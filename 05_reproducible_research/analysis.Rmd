---
title: "Analysis of NOOA data <write better title>"
subtitle: "Write a good subtitle"
author: "Matheus Cardoso"
date: "May 28, 2020"
output: 
  html_document: 
    fig_caption: yes
    highlight: zenburn
    keep_md: yes
    theme: spacelab
    toc: yes
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache = TRUE, warning = FALSE, message = FALSE)
```

## Data Processing

Before starting even download the data, the enviroment is seted up.

```{r}
library(tidyverse)
library(magrittr)
library(data.table)
library(R.utils)
```
To perform this analysis, the data is downloaded from the coursera's [course project](https://www.coursera.org/learn/reproducible-research/peer/OMZ37/course-project-2).
Dataset is saved in `data/storm_data.csv.bz2`, and extracted to `data/storm_data.csv`.
Also, two files that contain explanation about the data are downloaded and save as
`data/storm_data_documentation.pdf` and 
`data/storm_data_faq.pdf`.

```{r}
# set paths and urls
stormdata_url      <- "https://d396qusza40orc.cloudfront.net/repdata%2Fdata%2FStormData.csv.bz2"
stormdata_path     <- "data/stormdata.csv.bz2"
stormdata_doc_url  <- "https://d396qusza40orc.cloudfront.net/repdata%2Fpeer2_doc%2Fpd01016005curr.pdf"
stormdata_doc_path <- "data/storm_data_documentation.pdf"
stormdata_faq_url  <- "https://d396qusza40orc.cloudfront.net/repdata%2Fpeer2_doc%2FNCDC%20Storm%20Events-FAQ%20Page.pdf"
stormdata_faq_path <- "data/storm_data_faq.pdf"

# create ./data folder if it does not exists yet.
if (!dir.exists(dirname(stormdata_path))) {
    dir.create(dirname(stormdata_path))
}

# download data file if it is not yet in disk
if (!file.exists(stormdata_path)) {
    download.file(stormdata_url, stormdata_path, "curl")
}

# download documentation file if it is not yet in disk
if (!file.exists(stormdata_doc_path)) {
    download.file(stormdata_doc_url, stormdata_doc_path, "curl")
}

# download FAQ file if it is not yet in disk
if (!file.exists(stormdata_faq_path)) {
    download.file(stormdata_faq_url, stormdata_faq_path, "curl")
}

# after download, data is extracted
stormdata_file <- "data/stormdata.csv"
if (!file.exists(stormdata_file)) {
   bunzip2(filename = stormdata_path,
           destname = stormdata_file,
           remove   = FALSE) 
}
```

After download, data is loaded into `R` session.
I opted to create a binary file in `.rds` format to avoid parse the raw `.csv` file every time.
This file is located at `data/stormdata.rds`

```{r}
# Load data

## check if the parsed binary exists.
## If doesn't exist, parse raw data
stormdata_parsed <- "data/stormdata.rds"

if(!file.exists(stormdata_parsed)) {
    storm <- read_csv(stormdata_file,
        trim_ws = TRUE,
        col_types = cols_only(
            STATE      = "c",
            EVTYPE     = "c",
            FATALITIES = "d",
            INJURIES   = "d",
            PROPDMG    = "d",
            PROPDMGEXP = "c",
            CROPDMG    = "d",
            CROPDMGEXP = "c"))
    write_rds(storm, stormdata_parsed)
} else {
    storm <- read_rds(stormdata_parsed)
}
```


After load data, the colunms are modified to a more tidy format.
In this dataset, suffix like `K`, `M` and `B` where used to express expoents.
`r 1E3`, `r 1E6` and `r 1E9` respectively.

```{r}
storm %<>% mutate(PROPDMGEXP = case_when(str_to_lower(PROPDMGEXP) == "k" ~ 1E3,
                                        str_to_lower(PROPDMGEXP) == "m" ~ 1E6,
                                        str_to_lower(PROPDMGEXP) == "b" ~ 1E9,
                                        str_detect(
                                            str_to_lower(PROPDMGEXP), "^[0-9]$") ~ 10^as.numeric(PROPDMGEXP)))
                                        
storm %<>% mutate(CROPDMGEXP = case_when(str_to_lower(CROPDMGEXP) == "k" ~ 1E3,
                                        str_to_lower(CROPDMGEXP) == "m" ~ 1E6,
                                        str_to_lower(CROPDMGEXP) == "b" ~ 1E9,
                                        str_detect(
                                            str_to_lower(CROPDMGEXP), "^[0-9]$") ~ 10^as.numeric(CROPDMGEXP)))
                                        
storm %<>%
    mutate(PROPDMG = PROPDMG * PROPDMGEXP,
                 CROPDMG = CROPDMG * CROPDMGEXP) %>% 
    select(-ends_with("EXP"))
```

After processing, the dataframe looks like this:

```{r storm_table, include=T, echo=F}
knitr::kable(head(storm), caption = "A tidy dataframe", booktabs = TRUE)
```

```{r eval=F, include=F}
# some EDA
# df <- df %>% group_by(EVTYPE)

# df_summary <- df %>% summarise(health = sum(FATALITIES, INJURIES)) %>% arrange(desc(health))
# 
# write_rds(df, "data/parsed_df.rds")
# 
# a <- read_rds("data/parsed_df.rds")
# df_summary
# a <- df_summary[1:10, ]
# ggplot(a) + geom_bar(aes(str_to_lower(EVTYPE), weight = health)) + theme(axis.text.x = element_text(angle = 90))
```

process the data to find common event types

```{r eval=TRUE, include=TRUE}

common_types = c("hail", "wind", "flood", "lightning", "snow",
                 "rain", "storm", "weather", "cloud", "waterspout",
                 "wildfire", "blizzard", "drought", "heat", "tornado",
                 "hurricane", "cold", "freeze", "fog", "stream",
                 "fire", "funnel", "glaze", "temperature")
ham <- storm
for (i in common_types) {
    if (! "type" %in% colnames(ham)) {
        ham <- add_column(ham, type = NA)
    }
    ham <- mutate(ham, type = ifelse(str_detect(str_to_lower(EVTYPE), glue::glue(".*{i}.*")), glue::glue("{i}"), type))
}
# df %>% mutate(type = ifelse(str_detect(str_to_lower(EVTYPE), ".*wind.*"), "wind", str_to_lower(EVTYPE)))
# df %>% mutate(type = ifelse(str_detect(str_to_lower(EVTYPE), ".*wind.*"), "wind", str_to_lower(EVTYPE)))
# df %>% count(EVTYPE) %>% arrange(desc(n))


spham <- ham %>% group_by(type) %>% summarise(health = sum(FATALITIES, INJURIES)) %>% arrange(desc(health))

ggplot(spham) + geom_bar(aes(str_to_lower(type), weight = health)) + theme(axis.text.x = element_text(angle = 90)) + scale_x_discrete(limits = spham$type)
```
